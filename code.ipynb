{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Let's install all require libraries. For example, `transformers`"
      ],
      "metadata": {
        "id": "_3ZWJlO6JOqY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "bqxPHsUOqVvh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9445ff48-e7c0-49b2-9565-0917c104eb94"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.14.0-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.2/224.2 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.11.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m49.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.4.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.14.0 tokenizers-0.13.3 transformers-4.28.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's import all require libraries. \n",
        "For example, `numpy`"
      ],
      "metadata": {
        "id": "U5XEt6asIi3Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import string\n",
        "import numpy\n",
        "import io\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "import pickle "
      ],
      "metadata": {
        "id": "TKEZRYhIImbg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7375fa9f-3a3b-4c3b-fa85-20ad1ecad760"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UCt8HdFDdfcw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Let's put  student id as a variable, that  will use in different places**"
      ],
      "metadata": {
        "id": "pd5kSsAPZoE6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "student_id = 2201735"
      ],
      "metadata": {
        "id": "rqP6pp_3ZkVy"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's set `seed` for all libraries like `torch`, `numpy` etc as my student id"
      ],
      "metadata": {
        "id": "RiLUrQ-3zC6V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set same seeds for all libraries\n",
        "\n",
        "#numpy seed\n",
        "np.random.seed(student_id)"
      ],
      "metadata": {
        "id": "TYUn2tj3zCFq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Dlj_VQrkbLgM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "uOESFmIPn_nr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "metadata": {
        "id": "L1kvIe1NbDoS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b5ad353-7594-4198-f4cc-acd8f95d259a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add your code to initialize GDrive and data and models paths\n",
        "\n",
        "# TODO: Fill in the Google Drive path where you uploaded the assignment, data and code\n",
        "# Example: If your student_id is 1234567 then your directory will be './CE807/Assignment2/1234567/' \n",
        "\n",
        "GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = os.path.join('./CE807/Assignment2/',str(student_id)) # Make sure to update with your student_id and student_id is an integer\n",
        "GOOGLE_DRIVE_PATH = os.path.join('gdrive', 'MyDrive', GOOGLE_DRIVE_PATH_AFTER_MYDRIVE)\n",
        "print('List files: ', os.listdir(GOOGLE_DRIVE_PATH))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-WKnbP3roLTj",
        "outputId": "59976124-7d76-4224-b00f-3856b9307a41"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "List files:  ['models', 'test.csv', 'valid.csv', 'train.csv', 'train_25.csv', 'train_50.csv', 'train_75.csv', 'train_100.csv']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aai8l0zugFtU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5iLzVcPIgGjh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's get all file names"
      ],
      "metadata": {
        "id": "DdLem9mzWDQo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_file = os.path.join(GOOGLE_DRIVE_PATH, 'train.csv') # This is 100% of data\n",
        "df = pd.read_csv(train_file)\n",
        "df_split = np.array_split(df,4)\n",
        "Part_1=df_split[0]\n",
        "Part_2=df_split[1]\n",
        "Part_3=df_split[2]\n",
        "Part_4=df_split[3]\n",
        "train_25=Part_1\n",
        "train_50=pd.concat([Part_1,Part_2])\n",
        "train_75=pd.concat([Part_1,Part_2,Part_3])\n",
        "train_100=pd.concat([Part_1,Part_2,Part_3,Part_4])\n",
        "\n",
        "# Save each part as a separate CSV file\n",
        "train_25.to_csv('/content/gdrive/MyDrive/CE807/Assignment2/2201735/train_25.csv', index=False)\n",
        "train_50.to_csv('/content/gdrive/MyDrive/CE807/Assignment2/2201735/train_50.csv', index=False)\n",
        "train_75.to_csv('/content/gdrive/MyDrive/CE807/Assignment2/2201735/train_75.csv', index=False)\n",
        "train_100.to_csv('/content/gdrive/MyDrive/CE807/Assignment2/2201735/train_100.csv', index=False)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FaCVLw5xV-5A"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_25_file = os.path.join(GOOGLE_DRIVE_PATH, 'train_25.csv') #This is 25% of data\n",
        "train_50_file = os.path.join(GOOGLE_DRIVE_PATH, 'train_50.csv') #This is 50% of data\n",
        "train_75_file = os.path.join(GOOGLE_DRIVE_PATH, 'train_75.csv')  #This is 75% of data\n",
        "train_100_file = os.path.join(GOOGLE_DRIVE_PATH, 'train_100.csv')  #This is 100% of data\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print('Train 100% file: ', train_100_file)\n",
        "print('Train 25% file: ', train_25_file)\n",
        "print('Train 50% file: ', train_50_file)\n",
        "print('Train 75% file: ', train_75_file)\n",
        "\n",
        "val_file = os.path.join(GOOGLE_DRIVE_PATH, 'valid.csv')\n",
        "print('Validation file: ', val_file)\n",
        "\n",
        "test_file = os.path.join(GOOGLE_DRIVE_PATH, 'test.csv')\n",
        "print('Test file: ', test_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zFiYSZdAhidE",
        "outputId": "5360425e-b857-45e6-ea8a-40bbb82e9004"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train 100% file:  gdrive/MyDrive/./CE807/Assignment2/2201735/train_100.csv\n",
            "Train 25% file:  gdrive/MyDrive/./CE807/Assignment2/2201735/train_25.csv\n",
            "Train 50% file:  gdrive/MyDrive/./CE807/Assignment2/2201735/train_50.csv\n",
            "Train 75% file:  gdrive/MyDrive/./CE807/Assignment2/2201735/train_75.csv\n",
            "Validation file:  gdrive/MyDrive/./CE807/Assignment2/2201735/valid.csv\n",
            "Test file:  gdrive/MyDrive/./CE807/Assignment2/2201735/test.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's set output model directory and file names.\n",
        "\n"
      ],
      "metadata": {
        "id": "3fibMnjZWRtp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_1_DIRECTORY = os.path.join(GOOGLE_DRIVE_PATH, 'models', '1') # Model 1 directory\n",
        "print('Model 1 directory: ', MODEL_1_DIRECTORY)\n",
        "\n",
        "MODEL_1_25_DIRECTORY = os.path.join(MODEL_1_DIRECTORY,'25') # Model 1 trained using 25% of train data directory\n",
        "print('Model 1 directory with 25% data: ', MODEL_1_25_DIRECTORY)\n",
        "\n",
        "MODEL_1_50_DIRECTORY = os.path.join(MODEL_1_DIRECTORY,'50') # Model 1 trained using 50% of train data directory\n",
        "print('Model 1 directory with 50% data: ', MODEL_1_50_DIRECTORY)\n",
        "\n",
        "MODEL_1_75_DIRECTORY = os.path.join(MODEL_1_DIRECTORY,'75') # Model 1 trained using 75% of train data directory\n",
        "print('Model 1 directory with 75% data: ', MODEL_1_75_DIRECTORY)\n",
        "\n",
        "MODEL_1_100_DIRECTORY = os.path.join(MODEL_1_DIRECTORY,'100') # Model 1 trained using 100% of train data directory\n",
        "print('Model 1 directory with 100% data: ', MODEL_1_100_DIRECTORY)\n",
        "\n",
        "\n",
        "model_1_25_output_test_file = os.path.join(MODEL_1_25_DIRECTORY, 'output_test.csv') # Output file using Model 1 trained using 25% of train data \n",
        "print('Output file name using model 1 using 25% of train data: ',model_1_25_output_test_file)\n",
        "\n",
        "model_1_50_output_test_file = os.path.join(MODEL_1_50_DIRECTORY, 'output_test.csv') # Output file using Model 1 trained using 50% of train data \n",
        "print('Output file name using model 1 using 50% of train data: ',model_1_50_output_test_file)\n",
        "\n",
        "model_1_75_output_test_file = os.path.join(MODEL_1_75_DIRECTORY, 'output_test.csv') # Output file using Model 1 trained using 75% of train data \n",
        "print('Output file name using model 1 using 75% of train data: ',model_1_75_output_test_file)\n",
        "\n",
        "model_1_100_output_test_file = os.path.join(MODEL_1_100_DIRECTORY, 'output_test.csv') # Output file using Model 1 trained using 100% of train data \n",
        "print('Output file name using model 1 using 100% of train data: ',model_1_100_output_test_file)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQiwiE7BWXyQ",
        "outputId": "d24a4dbc-1837-4538-e271-76ed007d1c13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model 1 directory:  gdrive/MyDrive/./CE807/Assignment2/2201735/models/1\n",
            "Model 1 directory with 25% data:  gdrive/MyDrive/./CE807/Assignment2/2201735/models/1/25\n",
            "Model 1 directory with 50% data:  gdrive/MyDrive/./CE807/Assignment2/2201735/models/1/50\n",
            "Model 1 directory with 75% data:  gdrive/MyDrive/./CE807/Assignment2/2201735/models/1/75\n",
            "Model 1 directory with 100% data:  gdrive/MyDrive/./CE807/Assignment2/2201735/models/1/100\n",
            "Output file name using model 1 using 25% of train data:  gdrive/MyDrive/./CE807/Assignment2/2201735/models/1/25/output_test.csv\n",
            "Output file name using model 1 using 50% of train data:  gdrive/MyDrive/./CE807/Assignment2/2201735/models/1/50/output_test.csv\n",
            "Output file name using model 1 using 75% of train data:  gdrive/MyDrive/./CE807/Assignment2/2201735/models/1/75/output_test.csv\n",
            "Output file name using model 1 using 100% of train data:  gdrive/MyDrive/./CE807/Assignment2/2201735/models/1/100/output_test.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_2_DIRECTORY = os.path.join(GOOGLE_DRIVE_PATH, 'models', '2') # Model 2 directory\n",
        "print('Model 2 directory: ', MODEL_2_DIRECTORY)\n",
        "MODEL_2_25_DIRECTORY = os.path.join(MODEL_2_DIRECTORY,'25') # Model 2 trained using 25% of train data directory\n",
        "print('Model 2 directory with 25% data: ', MODEL_2_25_DIRECTORY)\n",
        "\n",
        "MODEL_2_50_DIRECTORY = os.path.join(MODEL_2_DIRECTORY,'50') # Model 2 trained using 50% of train data directory\n",
        "print('Model 2 directory with 50% data: ', MODEL_2_50_DIRECTORY)\n",
        "\n",
        "MODEL_2_75_DIRECTORY = os.path.join(MODEL_2_DIRECTORY,'75') # Model 2 trained using 75% of train data directory\n",
        "print('Model 2 directory with 75% data: ', MODEL_2_75_DIRECTORY)\n",
        "\n",
        "MODEL_2_100_DIRECTORY = os.path.join(MODEL_2_DIRECTORY,'100') # Model 2 trained using 100% of train data directory\n",
        "print('Model 2 directory with 100% data: ', MODEL_2_100_DIRECTORY)\n",
        "\n",
        "\n",
        "model_2_25_output_test_file = os.path.join(MODEL_2_25_DIRECTORY, 'output_test.csv') # Output file using Model 1 trained using 25% of train data \n",
        "print('Output file name using model 2 using 25% of train data: ',model_2_25_output_test_file)\n",
        "\n",
        "model_2_50_output_test_file = os.path.join(MODEL_2_50_DIRECTORY, 'output_test.csv') # Output file using Model 1 trained using 50% of train data \n",
        "print('Output file name using model 2 using 50% of train data: ',model_2_50_output_test_file)\n",
        "\n",
        "model_2_75_output_test_file = os.path.join(MODEL_2_75_DIRECTORY, 'output_test.csv') # Output file using Model 1 trained using 75% of train data \n",
        "print('Output file name using model 3 using 75% of train data: ',model_2_75_output_test_file)\n",
        "\n",
        "model_2_100_output_test_file = os.path.join(MODEL_2_100_DIRECTORY, 'output_test.csv') # Output file using Model 1 trained using 100% of train data \n",
        "print('Output file name using model 2 using 100% of train data: ',model_2_100_output_test_file)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4CbgfrWKiGhF",
        "outputId": "6766664a-11b8-4955-b529-f21c4123360f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model 2 directory:  gdrive/MyDrive/./CE807/Assignment2/2201735/models/2\n",
            "Model 2 directory with 25% data:  gdrive/MyDrive/./CE807/Assignment2/2201735/models/2/25\n",
            "Model 2 directory with 50% data:  gdrive/MyDrive/./CE807/Assignment2/2201735/models/2/50\n",
            "Model 2 directory with 75% data:  gdrive/MyDrive/./CE807/Assignment2/2201735/models/2/75\n",
            "Model 2 directory with 100% data:  gdrive/MyDrive/./CE807/Assignment2/2201735/models/2/100\n",
            "Output file name using model 2 using 25% of train data:  gdrive/MyDrive/./CE807/Assignment2/2201735/models/2/25/output_test.csv\n",
            "Output file name using model 2 using 50% of train data:  gdrive/MyDrive/./CE807/Assignment2/2201735/models/2/50/output_test.csv\n",
            "Output file name using model 3 using 75% of train data:  gdrive/MyDrive/./CE807/Assignment2/2201735/models/2/75/output_test.csv\n",
            "Output file name using model 2 using 100% of train data:  gdrive/MyDrive/./CE807/Assignment2/2201735/models/2/100/output_test.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Dw6o4z1-a3ZR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_3_DIRECTORY = os.path.join(GOOGLE_DRIVE_PATH, 'models', '3') # Model 3 directory\n",
        "print('Model 3 directory: ', MODEL_3_DIRECTORY)\n",
        "MODEL_3_25_DIRECTORY = os.path.join(MODEL_3_DIRECTORY,'25') # Model 3 trained using 25% of train data directory\n",
        "print('Model 3 directory with 25% data: ', MODEL_3_25_DIRECTORY)\n",
        "\n",
        "MODEL_3_50_DIRECTORY = os.path.join(MODEL_3_DIRECTORY,'50') # Model 3 trained using 50% of train data directory\n",
        "print('Model 3 directory with 50% data: ', MODEL_3_50_DIRECTORY)\n",
        "\n",
        "MODEL_3_75_DIRECTORY = os.path.join(MODEL_3_DIRECTORY,'75') # Model 3 trained using 75% of train data directory\n",
        "print('Model 3 directory with 75% data: ', MODEL_3_75_DIRECTORY)\n",
        "\n",
        "MODEL_3_100_DIRECTORY = os.path.join(MODEL_3_DIRECTORY,'100') # Model 3 trained using 100% of train data directory\n",
        "print('Model 3 directory with 100% data: ', MODEL_3_100_DIRECTORY)\n",
        "\n",
        "\n",
        "model_3_25_output_test_file = os.path.join(MODEL_3_25_DIRECTORY, 'output_test.csv') # Output file using Model 1 trained using 25% of train data \n",
        "print('Output file name using model 3 using 25% of train data: ',model_3_25_output_test_file)\n",
        "\n",
        "model_3_50_output_test_file = os.path.join(MODEL_3_50_DIRECTORY, 'output_test.csv') # Output file using Model 1 trained using 50% of train data \n",
        "print('Output file name using model 3 using 50% of train data: ',model_3_50_output_test_file)\n",
        "\n",
        "model_3_75_output_test_file = os.path.join(MODEL_3_75_DIRECTORY, 'output_test.csv') # Output file using Model 1 trained using 75% of train data \n",
        "print('Output file name using model 3 using 75% of train data: ',model_3_75_output_test_file)\n",
        "\n",
        "model_3_100_output_test_file = os.path.join(MODEL_3_100_DIRECTORY, 'output_test.csv') # Output file using Model 1 trained using 100% of train data \n",
        "print('Output file name using model 3 using 100% of train data: ',model_3_100_output_test_file)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X-DczSv7a34E",
        "outputId": "166401f6-275f-408e-8f8d-3c712fdce962"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model 3 directory:  gdrive/MyDrive/./CE807/Assignment2/2201735/models/3\n",
            "Model 3 directory with 25% data:  gdrive/MyDrive/./CE807/Assignment2/2201735/models/3/25\n",
            "Model 3 directory with 50% data:  gdrive/MyDrive/./CE807/Assignment2/2201735/models/3/50\n",
            "Model 3 directory with 75% data:  gdrive/MyDrive/./CE807/Assignment2/2201735/models/3/75\n",
            "Model 3 directory with 100% data:  gdrive/MyDrive/./CE807/Assignment2/2201735/models/3/100\n",
            "Output file name using model 3 using 25% of train data:  gdrive/MyDrive/./CE807/Assignment2/2201735/models/3/25/output_test.csv\n",
            "Output file name using model 3 using 50% of train data:  gdrive/MyDrive/./CE807/Assignment2/2201735/models/3/50/output_test.csv\n",
            "Output file name using model 3 using 75% of train data:  gdrive/MyDrive/./CE807/Assignment2/2201735/models/3/75/output_test.csv\n",
            "Output file name using model 3 using 100% of train data:  gdrive/MyDrive/./CE807/Assignment2/2201735/models/3/100/output_test.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see train file"
      ],
      "metadata": {
        "id": "0pWvXDghtafa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(train_file)\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "dptNCuqjq5k_",
        "outputId": "29aaf139-af0e-4ea0-954f-3e791120f50d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      id                                              tweet label\n",
              "0  42884  @USER I’m done with you as well. An INTENTIONA...   NOT\n",
              "1  92152  I now have over 6k followers.  Only 94k to go ...   NOT\n",
              "2  65475  @USER Tom was bought! He is more interested in...   NOT\n",
              "3  22144  @USER @USER Even her brother thinks she is a m...   OFF\n",
              "4  81048  @USER @USER @USER @USER @USER I can understand...   OFF"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-05c6620b-d3b5-4a05-ad6e-89b0499233c0\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>tweet</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>42884</td>\n",
              "      <td>@USER I’m done with you as well. An INTENTIONA...</td>\n",
              "      <td>NOT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>92152</td>\n",
              "      <td>I now have over 6k followers.  Only 94k to go ...</td>\n",
              "      <td>NOT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>65475</td>\n",
              "      <td>@USER Tom was bought! He is more interested in...</td>\n",
              "      <td>NOT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>22144</td>\n",
              "      <td>@USER @USER Even her brother thinks she is a m...</td>\n",
              "      <td>OFF</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>81048</td>\n",
              "      <td>@USER @USER @USER @USER @USER I can understand...</td>\n",
              "      <td>OFF</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-05c6620b-d3b5-4a05-ad6e-89b0499233c0')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-05c6620b-d3b5-4a05-ad6e-89b0499233c0 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-05c6620b-d3b5-4a05-ad6e-89b0499233c0');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are going to use different performance matrics like Accuracy, Recall (macro), Precision (macro), F1 (macro) and Confusion Matrix for the performance evaluation. We will print all the matrics and display Confusion Matrix with proper X & Y axis labels"
      ],
      "metadata": {
        "id": "E-OjJ4REbhcj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_performance(y_true, y_pred, split='test'):\n",
        "    \"\"\"\n",
        "    prints different performance matrics like  Accuracy, Recall (macro), Precision (macro), and F1 (macro).\n",
        "    This also display Confusion Matrix with proper X & Y axis labels.\n",
        "    Also, returns F1 score\n",
        "\n",
        "    Args:\n",
        "        y_true: numpy array or list\n",
        "        y_pred: numpy array or list\n",
        "        split: str\n",
        "        \n",
        "\n",
        "    Returns:\n",
        "        float\n",
        "    \"\"\"\n",
        "\n",
        "    print('Computing different preformance metrics on', split, ' set of Dataset')\n",
        "    f1score=f1_score(y_true, y_pred, average='macro')\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    \n",
        "    print('F1 Score(macro): ', f1score)\n",
        "    print('Accuracy: ', acc)\n",
        "\n",
        "    return f1score"
      ],
      "metadata": {
        "id": "aLuUu5BWcTid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#Method 1 /Model1 Start"
      ],
      "metadata": {
        "id": "47ywe8jGSKhL"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z-bthsb3SX6o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training for Method1 code\n"
      ],
      "metadata": {
        "id": "1sA3OWlVbnoY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to vectorizing the data, data cleaning and feature extraction"
      ],
      "metadata": {
        "id": "BJyIgPsAapum"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_dataset1(data, Tfid_vectorizer=None, split='test'):\n",
        "  #Missing values checking\n",
        "  missing_values = data.isnull().sum()\n",
        "  #Print the number of missing values for each column\n",
        "  print(missing_values)\n",
        "\n",
        "  #1.Punctuation Removal\n",
        "  string.punctuation\n",
        "  #defining the function to remove punctuation\n",
        "  def remove_punctuation(data):\n",
        "    punctuationfree=\"\".join([i for i in data if i not in string.punctuation])\n",
        "    return punctuationfree\n",
        "  #storing the puntuation free text\n",
        "  df['tweet']= df['tweet'].apply(lambda x:remove_punctuation(x))\n",
        "\n",
        "\n",
        "  #2.Lowering Text\n",
        "  df['tweet']= df['tweet'].apply(lambda x: x.lower())\n",
        "\n",
        "\n",
        "  #3.Removing Numbers\n",
        "  df['tweet'] = df['tweet'].str.replace('\\d+', '')\n",
        "  import re\n",
        "  emoji_pattern = re.compile(\"[\"\n",
        "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "  df['tweet'] = df['tweet'].apply(lambda x: emoji_pattern.sub(r'', x))\n",
        "\n",
        "\n",
        "  #4.Stopwords Removal\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  def remove_stop_words(tweet):\n",
        "    tokens = word_tokenize(tweet)\n",
        "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "    filtered_text = ' '.join(filtered_tokens)\n",
        "    return filtered_text\n",
        "  df['tweet'] = df['tweet'].apply(remove_stop_words)\n",
        "\n",
        "\n",
        "  #5.Tokenization\n",
        "  df['tweet']=df['tweet'].apply(str)\n",
        "  df['tweet'] = df['tweet'].apply(lambda x: word_tokenize(x))\n",
        "\n",
        "\n",
        "  \n",
        "  #6. Word Vectorization Using TfidfVectorizer\n",
        "  X=df['tweet'].apply(str)\n",
        "  if split == 'train':\n",
        "      Tfid_vectorizer  = TfidfVectorizer()\n",
        "      values = Tfid_vectorizer.fit_transform(X)\n",
        "\n",
        "  else:\n",
        "      values = Tfid_vectorizer.transform(data['tweet'].apply(str))\n",
        "\n",
        "  if split == 'train':\n",
        "      return values, Tfid_vectorizer\n",
        "  else:\n",
        "      return values"
      ],
      "metadata": {
        "id": "Dkzfmz1VapS-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linear SVC training"
      ],
      "metadata": {
        "id": "BtXYf8trgBM-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model1(text_vector,label):\n",
        "  Clasifier_svm = LinearSVC(max_iter=100)\n",
        "  Clasifier_svm.fit(text_vector, label)\n",
        "  return Clasifier_svm\n"
      ],
      "metadata": {
        "id": "mqv3isARdDek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save model and count vectorizer as pickel in GDrive"
      ],
      "metadata": {
        "id": "6aHXeTabgDGN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_model1(model, vectorizer, model_dir):\n",
        "    # save the model to disk\n",
        "    model_file = os.path.join(model_dir, 'model.sav')\n",
        "    pickle.dump(model, open(model_file, 'wb'))\n",
        "\n",
        "    print('Saved model to ', model_file)\n",
        "\n",
        "    vectorizer_file = os.path.join(model_dir, 'vectorizer.sav') \n",
        "    pickle.dump(vectorizer, open(vectorizer_file, 'wb'))\n",
        "\n",
        "    print('Saved Vectorizer to ', vectorizer_file)\n",
        "\n",
        "    return model_file, vectorizer_file "
      ],
      "metadata": {
        "id": "NX5Q4BYLgAko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load model and count vectorizer  as pickel from GDrive"
      ],
      "metadata": {
        "id": "3Pk6_0AkghWX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model1(model_file, vectorizer_file):\n",
        "    # load model and vectorizer from disk\n",
        "\n",
        "    model = pickle.load(open(model_file, 'rb'))\n",
        "\n",
        "    print('Loaded model from ', model_file)\n",
        "\n",
        "    vectorizer = pickle.load(open(vectorizer_file, 'rb'))\n",
        "\n",
        "    print('Loaded Vectorizer from ', vectorizer_file)\n",
        "\n",
        "\n",
        "    return model, vectorizer"
      ],
      "metadata": {
        "id": "zy0WpA4jgmoa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_method1(train_file, val_file, model_dir):\n",
        "    \"\"\"\n",
        "     Takes train_file, val_file and model_dir as input.\n",
        "     It trained on the train_file datapoints, and validate on the val_file datapoints.\n",
        "     While training and validating, it print different evaluataion metrics and losses, wheverever necessary.\n",
        "     After finishing the training, it saved the best model in the model_dir.\n",
        "\n",
        "     ADD Other arguments, if needed.\n",
        "\n",
        "    Args:\n",
        "        train_file: Train file name\n",
        "        val_file: Validation file name\n",
        "        model_dir: Model output Directory\n",
        "    \n",
        "    \"\"\"\n",
        "    train_df = pd.read_csv(train_file)\n",
        "    val_df = pd.read_csv(val_file)\n",
        "\n",
        "    train_label = train_df['label']\n",
        "    val_label = val_df['label']\n",
        "\n",
        "    train_values, Tfid_vectorizer = prepare_dataset1(train_df, split='train') \n",
        "    val_values= prepare_dataset1(val_df,Tfid_vectorizer)\n",
        "\n",
        "    model = train_model1(train_values,train_label)\n",
        "\n",
        "    model_file, vectorizer_file = save_model1(model, Tfid_vectorizer, model_dir)\n",
        "\n",
        "    train_pred_label = model.predict(train_values)\n",
        "    val_pred_label = model.predict(val_values)\n",
        "\n",
        "    # print('Train Split')\n",
        "    train_f1_score = compute_performance(train_label, train_pred_label, split='train')\n",
        "\n",
        "    # print('Validation Split')\n",
        "    val_f1_score = compute_performance(val_label, val_pred_label, split='valid')\n",
        "\n",
        "\n",
        "    return model_file, vectorizer_file"
      ],
      "metadata": {
        "id": "_Ux7GBKXbqq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's train using 100% of data"
      ],
      "metadata": {
        "id": "LRiNC3KmWq8C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Train using of 100% of data')\n",
        "model_100_file, vectorizer_100_file = train_method1(train_100_file, val_file, MODEL_1_100_DIRECTORY)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Csv_SZ5LcRWY",
        "outputId": "60753a01-da09-44be-bad6-07962dbb92d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train using of 100% of data\n",
            "id       0\n",
            "tweet    0\n",
            "label    0\n",
            "dtype: int64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-afbd2eb90332>:22: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  df['tweet'] = df['tweet'].str.replace('\\d+', '')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "id       0\n",
            "tweet    0\n",
            "label    0\n",
            "dtype: int64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-afbd2eb90332>:22: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  df['tweet'] = df['tweet'].str.replace('\\d+', '')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved model to  gdrive/MyDrive/./CE807/Assignment2/2201735/models/1/100/model.sav\n",
            "Saved Vectorizer to  gdrive/MyDrive/./CE807/Assignment2/2201735/models/1/100/vectorizer.sav\n",
            "Computing different preformance metrics on train  set of Dataset\n",
            "F1 Score(macro):  0.9565407345972785\n",
            "Accuracy:  0.961910176236498\n",
            "Computing different preformance metrics on valid  set of Dataset\n",
            "F1 Score(macro):  0.6783164121164649\n",
            "Accuracy:  0.7400215749730313\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's train using 25% of data. \n",
        "\n"
      ],
      "metadata": {
        "id": "JwyiYc7aXK5i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Train using of 25% of data')\n",
        "model_25_file, vectorizer_25_file = train_method1(train_25_file, val_file, MODEL_1_25_DIRECTORY)"
      ],
      "metadata": {
        "id": "yoEvlMP7Z5LC",
        "outputId": "d39a6423-026e-4de3-b44c-14b32277edc4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 574
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train using of 25% of data\n",
            "id       0\n",
            "tweet    0\n",
            "label    0\n",
            "dtype: int64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-afbd2eb90332>:22: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  df['tweet'] = df['tweet'].str.replace('\\d+', '')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "id       0\n",
            "tweet    0\n",
            "label    0\n",
            "dtype: int64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-afbd2eb90332>:22: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  df['tweet'] = df['tweet'].str.replace('\\d+', '')\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-47947aa746d2>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Train using of 25% of data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel_25_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectorizer_25_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_method1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_25_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMODEL_1_25_DIRECTORY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-17-5fc3293769f0>\u001b[0m in \u001b[0;36mtrain_method1\u001b[0;34m(train_file, val_file, model_dir)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mval_values\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mprepare_dataset1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mTfid_vectorizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mmodel_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectorizer_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msave_model1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTfid_vectorizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-c61f18cbedbb>\u001b[0m in \u001b[0;36mtrain_model1\u001b[0;34m(text_vector, label)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_model1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_vector\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mClasifier_svm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLinearSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mClasifier_svm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_vector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mClasifier_svm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/svm/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m         X, y = self._validate_data(\n\u001b[0m\u001b[1;32m    264\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    582\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"y\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 584\u001b[0;31m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    585\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmulti_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_numeric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_numeric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    395\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 397\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    398\u001b[0m             \u001b[0;34m\"Found input variables with inconsistent numbers of samples: %r\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m             \u001b[0;34m%\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [12313, 3079]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's train using 50% of data. "
      ],
      "metadata": {
        "id": "2mWrsHr7SQ6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Train using of 50% of data')\n",
        "model_50_file, vectorizer_50_file = train_method1(train_50_file, val_file, MODEL_1_50_DIRECTORY)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qjmelLVijyW8",
        "outputId": "71f81779-dc7d-42f8-ba14-f49656736c3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train using of 50% of data\n",
            "Saved model to  gdrive/MyDrive/./CE807/Assignment2/2201735/models/1/50/model.sav\n",
            "Saved Vectorizer to  gdrive/MyDrive/./CE807/Assignment2/2201735/models/1/50/vectorizer.sav\n",
            "Computing different preformance metrics on train  set of Dataset\n",
            "F1 Score(macro):  0.9698945464678116\n",
            "Accuracy:  0.9738509014130258\n",
            "Computing different preformance metrics on valid  set of Dataset\n",
            "F1 Score(macro):  0.6806235936305596\n",
            "Accuracy:  0.7238403451995685\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's train using 75% of data. "
      ],
      "metadata": {
        "id": "jc3AQ0PoSeRi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Train using of 75% of data')\n",
        "model_75_file, vectorizer_75_file = train_method1(train_75_file, val_file, MODEL_1_75_DIRECTORY)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KmWqmmo1kGFq",
        "outputId": "1f9cade3-2b59-498a-a650-e22c0672f62c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train using of 75% of data\n",
            "Saved model to  gdrive/MyDrive/./CE807/Assignment2/2201735/models/1/75/model.sav\n",
            "Saved Vectorizer to  gdrive/MyDrive/./CE807/Assignment2/2201735/models/1/75/vectorizer.sav\n",
            "Computing different preformance metrics on train  set of Dataset\n",
            "F1 Score(macro):  0.9419129947824464\n",
            "Accuracy:  0.9496480779642664\n",
            "Computing different preformance metrics on valid  set of Dataset\n",
            "F1 Score(macro):  0.6969704337677495\n",
            "Accuracy:  0.7389428263214671\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing code for Method1\n"
      ],
      "metadata": {
        "id": "qyJ_xv12Uy9c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_method1(test_file, model_file, vectorizer_file, output_dir):\n",
        "    \"\"\"\n",
        "     take test_file, model_file and output_dir as input.\n",
        "     It loads model and test of the examples in the test_file.\n",
        "     It prints different evaluation metrics, and saves the output in output directory\n",
        "\n",
        "     ADD Other arguments, if needed\n",
        "\n",
        "    Args:\n",
        "        test_file: Test file name\n",
        "        model_file: Model file name\n",
        "        vectorizer_file: Vectorizer file name\n",
        "        output_dir: Output Directory\n",
        "    \n",
        "    \"\"\"\n",
        "\n",
        "    test_df = pd.read_csv(test_file)\n",
        "    \n",
        "    test_label = test_df['label']\n",
        "\n",
        "    model, vectorizer = load_model1(model_file, vectorizer_file) \n",
        "\n",
        "    test_values= prepare_dataset1(test_df,vectorizer)\n",
        "\n",
        "    test_pred_label = model.predict(test_values)\n",
        "\n",
        "    test_df['out_label']  = test_pred_label # Note how this is saved \n",
        "\n",
        "    test_f1_score = compute_performance(test_label, test_pred_label, split='test')\n",
        "\n",
        "    out_file = os.path.join(output_dir, 'output_test.csv')\n",
        "\n",
        "    print('Saving model output to', out_file)\n",
        "    test_df.to_csv(out_file)\n",
        "\n",
        "    \n",
        "    # return "
      ],
      "metadata": {
        "id": "43T3JqK5a484"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's test using model trained on 100% data"
      ],
      "metadata": {
        "id": "Qd-JunUfXdVK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Testing using model trained on 100% data')\n",
        "test_method1(test_file, model_100_file, vectorizer_100_file, MODEL_1_100_DIRECTORY)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I0tXyGTyiDt2",
        "outputId": "f26e4f9d-3e27-445d-a961-023476bb014a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing using model trained on 100% data\n",
            "Loaded model from  gdrive/MyDrive/./CE807/Assignment2/2201735/models/1/100/model.sav\n",
            "Loaded Vectorizer from  gdrive/MyDrive/./CE807/Assignment2/2201735/models/1/100/vectorizer.sav\n",
            "Computing different preformance metrics on test  set of Dataset\n",
            "F1 Score(macro):  0.7018212356107054\n",
            "Accuracy:  0.7686046511627908\n",
            "Saving model output to gdrive/MyDrive/./CE807/Assignment2/2201735/models/1/100/output_test.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's test using model trained on 25% data"
      ],
      "metadata": {
        "id": "XjowbJuBStVG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Testing using model trained on 25% data')\n",
        "test_method1(test_file, model_25_file, vectorizer_25_file, MODEL_1_25_DIRECTORY)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YRe_JVi_kdwr",
        "outputId": "7deaa4b8-faae-4455-f9ff-178d92c43d1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing using model trained on 25% data\n",
            "Loaded model from  gdrive/MyDrive/./CE807/Assignment2/2201735/models/1/25/model.sav\n",
            "Loaded Vectorizer from  gdrive/MyDrive/./CE807/Assignment2/2201735/models/1/25/vectorizer.sav\n",
            "Computing different preformance metrics on test  set of Dataset\n",
            "F1 Score(macro):  0.671853520596706\n",
            "Accuracy:  0.7453488372093023\n",
            "Saving model output to gdrive/MyDrive/./CE807/Assignment2/2201735/models/1/25/output_test.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's test using model trained on 50% data"
      ],
      "metadata": {
        "id": "h-FtCtKzS4VP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Testing using model trained on 50% data')\n",
        "test_method1(test_file, model_50_file, vectorizer_50_file, MODEL_1_50_DIRECTORY)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2fvSBCOZkeH7",
        "outputId": "04142b3e-a94d-4522-d52d-5bc562c80ce1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing using model trained on 50% data\n",
            "Loaded model from  gdrive/MyDrive/./CE807/Assignment2/2201735/models/1/50/model.sav\n",
            "Loaded Vectorizer from  gdrive/MyDrive/./CE807/Assignment2/2201735/models/1/50/vectorizer.sav\n",
            "Computing different preformance metrics on test  set of Dataset\n",
            "F1 Score(macro):  0.6877209746774964\n",
            "Accuracy:  0.7569767441860465\n",
            "Saving model output to gdrive/MyDrive/./CE807/Assignment2/2201735/models/1/50/output_test.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's test using model trained on 75% data"
      ],
      "metadata": {
        "id": "USxKOf6DTAZb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Testing using model trained on 75% data')\n",
        "test_method1(test_file, model_75_file, vectorizer_75_file, MODEL_1_75_DIRECTORY)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ps03gTc6keac",
        "outputId": "7ca5cc62-7c06-4252-8941-f9998e7f2d1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing using model trained on 75% data\n",
            "Loaded model from  gdrive/MyDrive/./CE807/Assignment2/2201735/models/1/75/model.sav\n",
            "Loaded Vectorizer from  gdrive/MyDrive/./CE807/Assignment2/2201735/models/1/75/vectorizer.sav\n",
            "Computing different preformance metrics on test  set of Dataset\n",
            "F1 Score(macro):  0.6835170758760813\n",
            "Accuracy:  0.7523255813953489\n",
            "Saving model output to gdrive/MyDrive/./CE807/Assignment2/2201735/models/1/75/output_test.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "uatXpm4HXm8y"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dlAQo-8DXo3p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Method 1 End\n"
      ],
      "metadata": {
        "id": "ue3xIDFGSXNH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Method2/model2 Start\n"
      ],
      "metadata": {
        "id": "rmaJfJkVwSDW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training for Method2 Start"
      ],
      "metadata": {
        "id": "Dk0HI3K8TjHI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model2(text_vector,label):\n",
        "  Clasifier_sgdc = SGDClassifier(loss=\"hinge\", penalty=\"l2\", max_iter=50)\n",
        "  Clasifier_sgdc.fit(text_vector, label)\n",
        "  return Clasifier_sgdc"
      ],
      "metadata": {
        "id": "LQTmVWa5wdD3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save Model for Method2 and return vectorizer and model file"
      ],
      "metadata": {
        "id": "1SZO-yJeTyl5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_model2(model, vectorizer, model_dir):\n",
        "    # save the model to disk\n",
        "    model_file = os.path.join(model_dir, 'model.sav')\n",
        "    pickle.dump(model, open(model_file, 'wb'))\n",
        "\n",
        "    print('Saved model to ', model_file)\n",
        "\n",
        "    vectorizer_file = os.path.join(model_dir, 'vectorizer.sav') \n",
        "    pickle.dump(vectorizer, open(vectorizer_file, 'wb'))\n",
        "\n",
        "    print('Saved Vectorizer to ', vectorizer_file)\n",
        "\n",
        "    return model_file, vectorizer_file "
      ],
      "metadata": {
        "id": "6zBVQ4uplfzd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Model for Method2"
      ],
      "metadata": {
        "id": "QkPRPXJpT6fM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model2(model_file, vectorizer_file):\n",
        "    # load model and vectorizer from disk\n",
        "\n",
        "    model = pickle.load(open(model_file, 'rb'))\n",
        "\n",
        "    print('Loaded model from ', model_file)\n",
        "\n",
        "    vectorizer = pickle.load(open(vectorizer_file, 'rb'))\n",
        "\n",
        "    print('Loaded Vectorizer from ', vectorizer_file)\n",
        "\n",
        "\n",
        "    return model, vectorizer"
      ],
      "metadata": {
        "id": "E_7401qBlf8F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train Method for model2"
      ],
      "metadata": {
        "id": "cRVJJYaEUhe6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_method2(train_file, val_file, model_dir):\n",
        "    \"\"\"\n",
        "     Takes train_file, val_file and model_dir as input.\n",
        "     It trained on the train_file datapoints, and validate on the val_file datapoints.\n",
        "     While training and validating, it print different evaluataion metrics and losses, wheverever necessary.\n",
        "     After finishing the training, it saved the best model in the model_dir.\n",
        "\n",
        "     ADD Other arguments, if needed.\n",
        "\n",
        "    Args:\n",
        "        train_file: Train file name\n",
        "        val_file: Validation file name\n",
        "        model_dir: Model output Directory\n",
        "    \n",
        "    \"\"\"\n",
        "    train_df = pd.read_csv(train_file)\n",
        "    val_df = pd.read_csv(val_file)\n",
        "\n",
        "    train_label = train_df['label']\n",
        "    val_label = val_df['label']\n",
        "\n",
        "    train_values, count_vectorizer = prepare_dataset1(train_df, split='train') \n",
        "    val_values= prepare_dataset1(val_df,count_vectorizer)\n",
        "\n",
        "    model = train_model2(train_values,train_label)\n",
        "\n",
        "    model_file, vectorizer_file = save_model2(model, count_vectorizer, model_dir)\n",
        "\n",
        "    train_pred_label = model.predict(train_values)\n",
        "    val_pred_label = model.predict(val_values)\n",
        "\n",
        "    # print('Train Split')\n",
        "    train_f1_score = compute_performance(train_label, train_pred_label, split='train')\n",
        "\n",
        "    # print('Validation Split')\n",
        "    val_f1_score = compute_performance(val_label, val_pred_label, split='valid')\n",
        "\n",
        "\n",
        "    return model_file, vectorizer_file"
      ],
      "metadata": {
        "id": "DUxV4SsKlf_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train Using 100% data for Method2"
      ],
      "metadata": {
        "id": "78kDCB7WUuvL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Train using of 100% of data')\n",
        "model_100_file, vectorizer_100_file = train_method2(train_100_file, val_file, MODEL_2_100_DIRECTORY)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95-Eg71MlgF6",
        "outputId": "34319e45-7b9d-4d6d-8241-a2460b94c9a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train using of 100% of data\n",
            "Saved model to  gdrive/MyDrive/./CE807/Assignment2/2201735/models/2/100/model.sav\n",
            "Saved Vectorizer to  gdrive/MyDrive/./CE807/Assignment2/2201735/models/2/100/vectorizer.sav\n",
            "Computing different preformance metrics on train  set of Dataset\n",
            "F1 Score(macro):  0.7442993544596375\n",
            "Accuracy:  0.8106878908470722\n",
            "Computing different preformance metrics on valid  set of Dataset\n",
            "F1 Score(macro):  0.6447818434195274\n",
            "Accuracy:  0.7400215749730313\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train Using 25% data for Method2"
      ],
      "metadata": {
        "id": "lGfjh3xYU4Ji"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Train using of 25% of data')\n",
        "model_25_file, vectorizer_25_file = train_method2(train_25_file, val_file, MODEL_2_25_DIRECTORY)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "36e-d6J6lgJG",
        "outputId": "40ff7f80-d855-4fab-91e0-cc63fd62d549"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train using of 25% of data\n",
            "Saved model to  gdrive/MyDrive/./CE807/Assignment2/2201735/models/2/25/model.sav\n",
            "Saved Vectorizer to  gdrive/MyDrive/./CE807/Assignment2/2201735/models/2/25/vectorizer.sav\n",
            "Computing different preformance metrics on train  set of Dataset\n",
            "F1 Score(macro):  0.9809693204164146\n",
            "Accuracy:  0.9831113998051315\n",
            "Computing different preformance metrics on valid  set of Dataset\n",
            "F1 Score(macro):  0.6646773856698057\n",
            "Accuracy:  0.7141316073354909\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train Using 50% data for Method2"
      ],
      "metadata": {
        "id": "umks221jU-cs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Train using of 50% of data')\n",
        "model_50_file, vectorizer_50_file = train_method2(train_50_file, val_file, MODEL_2_50_DIRECTORY)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5IbDiNcqlgWe",
        "outputId": "5e5fedad-7cc0-4c82-a362-3742813ea407"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train using of 50% of data\n",
            "Saved model to  gdrive/MyDrive/./CE807/Assignment2/2201735/models/2/50/model.sav\n",
            "Saved Vectorizer to  gdrive/MyDrive/./CE807/Assignment2/2201735/models/2/50/vectorizer.sav\n",
            "Computing different preformance metrics on train  set of Dataset\n",
            "F1 Score(macro):  0.9398633829111185\n",
            "Accuracy:  0.9486763033945104\n",
            "Computing different preformance metrics on valid  set of Dataset\n",
            "F1 Score(macro):  0.6933484077633314\n",
            "Accuracy:  0.7443365695792881\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train Using 75% data for Method2"
      ],
      "metadata": {
        "id": "jJTfDJmVVC1i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Train using of 75% of data')\n",
        "model_75_file, vectorizer_75_file = train_method2(train_75_file, val_file, MODEL_2_75_DIRECTORY)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KwjJv6S9lga2",
        "outputId": "af8c7865-f9ce-4de0-8828-da58619b848d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train using of 75% of data\n",
            "Saved model to  gdrive/MyDrive/./CE807/Assignment2/2201735/models/2/75/model.sav\n",
            "Saved Vectorizer to  gdrive/MyDrive/./CE807/Assignment2/2201735/models/2/75/vectorizer.sav\n",
            "Computing different preformance metrics on train  set of Dataset\n",
            "F1 Score(macro):  0.9161839759296571\n",
            "Accuracy:  0.9275582024905252\n",
            "Computing different preformance metrics on valid  set of Dataset\n",
            "F1 Score(macro):  0.6936599284263469\n",
            "Accuracy:  0.7346278317152104\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing code for Method2"
      ],
      "metadata": {
        "id": "FPSN85y9oifs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Method defined"
      ],
      "metadata": {
        "id": "UX57lAHzVPWf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_method2(test_file, model_file, vectorizer_file, output_dir):\n",
        "    \"\"\"\n",
        "     take test_file, model_file and output_dir as input.\n",
        "     It loads model and test of the examples in the test_file.\n",
        "     It prints different evaluation metrics, and saves the output in output directory\n",
        "\n",
        "     ADD Other arguments, if needed\n",
        "\n",
        "    Args:\n",
        "        test_file: Test file name\n",
        "        model_file: Model file name\n",
        "        vectorizer_file: Vectorizer file name\n",
        "        output_dir: Output Directory\n",
        "    \n",
        "    \"\"\"\n",
        "\n",
        "    test_df = pd.read_csv(test_file)\n",
        "    \n",
        "    test_label = test_df['label']\n",
        "\n",
        "    model, vectorizer = load_model2(model_file, vectorizer_file) \n",
        "\n",
        "    test_values= prepare_dataset1(test_df,vectorizer)\n",
        "\n",
        "    test_pred_label = model.predict(test_values)\n",
        "\n",
        "    test_df['out_label']  = test_pred_label # Note how this is saved \n",
        "\n",
        "    test_f1_score = compute_performance(test_label, test_pred_label, split='test')\n",
        "\n",
        "    out_file = os.path.join(output_dir, 'output_test.csv')\n",
        "\n",
        "    print('Saving model output to', out_file)\n",
        "    test_df.to_csv(out_file)\n",
        "\n",
        "    \n",
        "    # return "
      ],
      "metadata": {
        "id": "3DFajGLLlgd1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's test using model trained on 100% data"
      ],
      "metadata": {
        "id": "mrujw9y5VU_T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Testing using model trained on 100% data')\n",
        "test_method2(test_file, model_100_file, vectorizer_100_file, MODEL_2_100_DIRECTORY)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DZbcg9mhlgg_",
        "outputId": "5d30dd1e-70cd-48eb-ef38-38412b2a8755"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing using model trained on 100% data\n",
            "Loaded model from  gdrive/MyDrive/./CE807/Assignment2/2201735/models/2/100/model.sav\n",
            "Loaded Vectorizer from  gdrive/MyDrive/./CE807/Assignment2/2201735/models/2/100/vectorizer.sav\n",
            "Computing different preformance metrics on test  set of Dataset\n",
            "F1 Score(macro):  0.7206924793131689\n",
            "Accuracy:  0.786046511627907\n",
            "Saving model output to gdrive/MyDrive/./CE807/Assignment2/2201735/models/2/100/output_test.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's test using model trained on 25% data"
      ],
      "metadata": {
        "id": "6Vm_2NwxVmI4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Testing using model trained on 25% data')\n",
        "test_method2(test_file, model_25_file, vectorizer_25_file, MODEL_2_25_DIRECTORY)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kAifFi0TpA72",
        "outputId": "c81456f7-c4a9-45b7-97ea-bec1e1e8f76c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing using model trained on 25% data\n",
            "Loaded model from  gdrive/MyDrive/./CE807/Assignment2/2201735/models/2/25/model.sav\n",
            "Loaded Vectorizer from  gdrive/MyDrive/./CE807/Assignment2/2201735/models/2/25/vectorizer.sav\n",
            "Computing different preformance metrics on test  set of Dataset\n",
            "F1 Score(macro):  0.6979113188683834\n",
            "Accuracy:  0.7581395348837209\n",
            "Saving model output to gdrive/MyDrive/./CE807/Assignment2/2201735/models/2/25/output_test.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's test using model trained on 50% data"
      ],
      "metadata": {
        "id": "XfOLv8xTVqGb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Testing using model trained on 50% data')\n",
        "test_method2(test_file, model_50_file, vectorizer_50_file, MODEL_2_50_DIRECTORY)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EXM8_QBzpBEt",
        "outputId": "f06e462f-6754-4dbf-d46e-9254044f9c4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing using model trained on 50% data\n",
            "Loaded model from  gdrive/MyDrive/./CE807/Assignment2/2201735/models/2/50/model.sav\n",
            "Loaded Vectorizer from  gdrive/MyDrive/./CE807/Assignment2/2201735/models/2/50/vectorizer.sav\n",
            "Computing different preformance metrics on test  set of Dataset\n",
            "F1 Score(macro):  0.683159833783147\n",
            "Accuracy:  0.75\n",
            "Saving model output to gdrive/MyDrive/./CE807/Assignment2/2201735/models/2/50/output_test.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's test using model trained on 75% data"
      ],
      "metadata": {
        "id": "s7qRWC5wVtqY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Testing using model trained on 75% data')\n",
        "test_method1(test_file, model_75_file, vectorizer_75_file, MODEL_2_75_DIRECTORY)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U5DCC5vzpBgV",
        "outputId": "5890cb9b-c546-4bd0-957d-01ac8fd2cf4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing using model trained on 75% data\n",
            "Loaded model from  gdrive/MyDrive/./CE807/Assignment2/2201735/models/2/75/model.sav\n",
            "Loaded Vectorizer from  gdrive/MyDrive/./CE807/Assignment2/2201735/models/2/75/vectorizer.sav\n",
            "Computing different preformance metrics on test  set of Dataset\n",
            "F1 Score(macro):  0.7349831697719622\n",
            "Accuracy:  0.7883720930232558\n",
            "Saving model output to gdrive/MyDrive/./CE807/Assignment2/2201735/models/2/75/output_test.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Method 2/model2 End"
      ],
      "metadata": {
        "id": "7yMswIeAwYIf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Method3/ Model3 Start"
      ],
      "metadata": {
        "id": "rvsiRDLaWAwe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "c6elN5CTWA9L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model3(text_vector, label):\n",
        "  #text_vector, label = make_classification(n_samples=1000, n_features=4, n_informative=2, n_redundant=0, random_state=0, shuffle=False)\n",
        "  clfr = RandomForestClassifier(max_depth=2, random_state=0)\n",
        "  clfr.fit(text_vector, label)\n",
        "  return clfr"
      ],
      "metadata": {
        "id": "jafZKx0HZIa4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_model3(model, vectorizer, model_dir):\n",
        "    # save the model to disk\n",
        "    model_file = os.path.join(model_dir, 'model.sav')\n",
        "    pickle.dump(model, open(model_file, 'wb'))\n",
        "\n",
        "    print('Saved model to ', model_file)\n",
        "\n",
        "    vectorizer_file = os.path.join(model_dir, 'vectorizer.sav') \n",
        "    pickle.dump(vectorizer, open(vectorizer_file, 'wb'))\n",
        "\n",
        "    print('Saved Vectorizer to ', vectorizer_file)\n",
        "\n",
        "    return model_file, vectorizer_file "
      ],
      "metadata": {
        "id": "tzh52bGpZJew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model3(model_file, vectorizer_file):\n",
        "    # load model and vectorizer from disk\n",
        "\n",
        "    model = pickle.load(open(model_file, 'rb'))\n",
        "\n",
        "    print('Loaded model from ', model_file)\n",
        "\n",
        "    vectorizer = pickle.load(open(vectorizer_file, 'rb'))\n",
        "\n",
        "    print('Loaded Vectorizer from ', vectorizer_file)\n",
        "\n",
        "\n",
        "    return model, vectorizer"
      ],
      "metadata": {
        "id": "vg_2di9sZJhq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_method3(train_file, val_file, model_dir):\n",
        "    \"\"\"\n",
        "     Takes train_file, val_file and model_dir as input.\n",
        "     It trained on the train_file datapoints, and validate on the val_file datapoints.\n",
        "     While training and validating, it print different evaluataion metrics and losses, wheverever necessary.\n",
        "     After finishing the training, it saved the best model in the model_dir.\n",
        "\n",
        "     ADD Other arguments, if needed.\n",
        "\n",
        "    Args:\n",
        "        train_file: Train file name\n",
        "        val_file: Validation file name\n",
        "        model_dir: Model output Directory\n",
        "    \n",
        "    \"\"\"\n",
        "    train_df = pd.read_csv(train_file)\n",
        "    val_df = pd.read_csv(val_file)\n",
        "\n",
        "    train_label = train_df['label']\n",
        "    val_label = val_df['label']\n",
        "\n",
        "    train_values, count_vectorizer = prepare_dataset1(train_df, split='train') \n",
        "    val_values= prepare_dataset1(val_df,count_vectorizer)\n",
        "\n",
        "    model = train_model3(train_values,train_label)\n",
        "\n",
        "    model_file, vectorizer_file = save_model3(model, count_vectorizer, model_dir)\n",
        "\n",
        "    train_pred_label = model.predict(train_values)\n",
        "    val_pred_label = model.predict(val_values)\n",
        "\n",
        "    # print('Train Split')\n",
        "    train_f1_score = compute_performance(train_label, train_pred_label, split='train')\n",
        "\n",
        "    # print('Validation Split')\n",
        "    val_f1_score = compute_performance(val_label, val_pred_label, split='valid')\n",
        "\n",
        "\n",
        "    return model_file, vectorizer_file"
      ],
      "metadata": {
        "id": "sTjSF4PVZJlH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Train using of 100% of data')\n",
        "model_100_file, vectorizer_100_file = train_method3(train_100_file, val_file, MODEL_3_100_DIRECTORY)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D5sGpZ2uZJn8",
        "outputId": "6fde012f-dda8-4e02-83d0-78506433b367"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train using of 100% of data\n",
            "id       0\n",
            "tweet    0\n",
            "label    0\n",
            "dtype: int64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-afbd2eb90332>:22: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  df['tweet'] = df['tweet'].str.replace('\\d+', '')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "id       0\n",
            "tweet    0\n",
            "label    0\n",
            "dtype: int64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-afbd2eb90332>:22: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  df['tweet'] = df['tweet'].str.replace('\\d+', '')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved model to  gdrive/MyDrive/./CE807/Assignment2/2201735/models/3/100/model.sav\n",
            "Saved Vectorizer to  gdrive/MyDrive/./CE807/Assignment2/2201735/models/3/100/vectorizer.sav\n",
            "Computing different preformance metrics on train  set of Dataset\n",
            "F1 Score(macro):  0.4003603779098081\n",
            "Accuracy:  0.6676683180378462\n",
            "Computing different preformance metrics on valid  set of Dataset\n",
            "F1 Score(macro):  0.40038809831824057\n",
            "Accuracy:  0.6677454153182308\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Train using of 100% of data')\n",
        "model_25_file, vectorizer_25_file = train_method3(train_25_file, val_file, MODEL_3_100_DIRECTORY)"
      ],
      "metadata": {
        "id": "Vl6xOwGAZJqe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 574
        },
        "outputId": "93c3d007-9cea-4b1a-8e9e-82a37bf4a483"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train using of 100% of data\n",
            "id       0\n",
            "tweet    0\n",
            "label    0\n",
            "dtype: int64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-afbd2eb90332>:22: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  df['tweet'] = df['tweet'].str.replace('\\d+', '')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "id       0\n",
            "tweet    0\n",
            "label    0\n",
            "dtype: int64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-afbd2eb90332>:22: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  df['tweet'] = df['tweet'].str.replace('\\d+', '')\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-96f529c7a77d>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Train using of 100% of data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel_25_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectorizer_25_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_method3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_25_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMODEL_3_100_DIRECTORY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-18-c747d55140f3>\u001b[0m in \u001b[0;36mtrain_method3\u001b[0;34m(train_file, val_file, model_dir)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mval_values\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mprepare_dataset1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcount_vectorizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mmodel_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectorizer_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msave_model3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount_vectorizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-a869a0ffc498>\u001b[0m in \u001b[0;36mtrain_model3\u001b[0;34m(text_vector, label)\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0;31m#text_vector, label = make_classification(n_samples=1000, n_features=4, n_informative=2, n_redundant=0, random_state=0, shuffle=False)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mclfr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mclfr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_vector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mclfr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    343\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sparse multilabel-indicator for y is not supported.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         X, y = self._validate_data(\n\u001b[0m\u001b[1;32m    346\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csc\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    582\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"y\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 584\u001b[0;31m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    585\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmulti_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_numeric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_numeric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    395\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 397\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    398\u001b[0m             \u001b[0;34m\"Found input variables with inconsistent numbers of samples: %r\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m             \u001b[0;34m%\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [12313, 3079]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_method3(test_file, model_file, vectorizer_file, output_dir):\n",
        "    \"\"\"\n",
        "     take test_file, model_file and output_dir as input.\n",
        "     It loads model and test of the examples in the test_file.\n",
        "     It prints different evaluation metrics, and saves the output in output directory\n",
        "\n",
        "     ADD Other arguments, if needed\n",
        "\n",
        "    Args:\n",
        "        test_file: Test file name\n",
        "        model_file: Model file name\n",
        "        vectorizer_file: Vectorizer file name\n",
        "        output_dir: Output Directory\n",
        "    \n",
        "    \"\"\"\n",
        "\n",
        "    test_df = pd.read_csv(test_file)\n",
        "    \n",
        "    test_label = test_df['label']\n",
        "\n",
        "    model, vectorizer = load_model3(model_file, vectorizer_file) \n",
        "\n",
        "    test_values= prepare_dataset1(test_df,vectorizer)\n",
        "\n",
        "    test_pred_label = model.predict(test_values)\n",
        "\n",
        "    test_df['out_label']  = test_pred_label # Note how this is saved \n",
        "\n",
        "    test_f1_score = compute_performance(test_label, test_pred_label, split='test')\n",
        "\n",
        "    out_file = os.path.join(output_dir, 'output_test.csv')\n",
        "\n",
        "    print('Saving model output to', out_file)\n",
        "    test_df.to_csv(out_file)\n",
        "\n",
        "    \n",
        "    # return \n",
        "\n",
        "    \n",
        "    "
      ],
      "metadata": {
        "id": "CbyYVFtjZJs-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Testing using model trained on 100% data')\n",
        "test_method3(test_file, model_100_file, vectorizer_100_file, MODEL_3_100_DIRECTORY)"
      ],
      "metadata": {
        "id": "WYRWJW86ZJv3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9db8497-8b9b-4a9d-e5c9-2d0645d7b67e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing using model trained on 100% data\n",
            "Loaded model from  gdrive/MyDrive/./CE807/Assignment2/2201735/models/3/100/model.sav\n",
            "Loaded Vectorizer from  gdrive/MyDrive/./CE807/Assignment2/2201735/models/3/100/vectorizer.sav\n",
            "id       0\n",
            "tweet    0\n",
            "label    0\n",
            "dtype: int64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-afbd2eb90332>:22: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  df['tweet'] = df['tweet'].str.replace('\\d+', '')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing different preformance metrics on test  set of Dataset\n",
            "F1 Score(macro):  0.4189189189189189\n",
            "Accuracy:  0.7209302325581395\n",
            "Saving model output to gdrive/MyDrive/./CE807/Assignment2/2201735/models/3/100/output_test.csv\n"
          ]
        }
      ]
    }
  ]
}